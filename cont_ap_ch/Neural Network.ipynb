{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dd39fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.stats import lognorm as lnorm\n",
    "from primitives import Parameters\n",
    "p = Parameters()\n",
    "import opt_einsum as oe\n",
    "from primitives import Preferences\n",
    "from probabilities import createPoissonTransitionMatrix,createBlockPoissonTransitionMatrix\n",
    "from search import JobSearchArray\n",
    "import matplotlib.pyplot as plt\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "def impose_decreasing(M):\n",
    "    nv = M.shape[1]\n",
    "    for v in reversed(range(nv-1)):\n",
    "        M[:,v,:] = np.maximum(M[:,v,:],M[:,v+1,:])\n",
    "    return M\n",
    "def impose_increasing(A0):\n",
    "    A = np.copy(A0)\n",
    "    nv = len(A)\n",
    "    for v in range(1,nv):\n",
    "        A[v] = np.maximum(A[v],A[v-1])\n",
    "    return A\n",
    "ax = np.newaxis\n",
    "class StateBoundsProcessor:\n",
    "    def __init__(self, lower_bounds, upper_bounds):\n",
    "        \"\"\"\n",
    "        Initialize with lower and upper bounds for each state dimension\n",
    "        \n",
    "        Args:\n",
    "            lower_bounds: List or tensor of lower bounds [x_1, x_2, ..., x_20]\n",
    "            upper_bounds: List or tensor of upper bounds [y_1, y_2, ..., y_20]\n",
    "        \"\"\"\n",
    "        self.lower_bounds = torch.tensor(lower_bounds, dtype=torch.float32)\n",
    "        self.upper_bounds = torch.tensor(upper_bounds, dtype=torch.float32)\n",
    "        self.range = self.upper_bounds - self.lower_bounds\n",
    "        \n",
    "    def normalize(self, states):\n",
    "        \"\"\"Scale states from [lower_bound, upper_bound] to [-1, 1]\"\"\"\n",
    "        return 2 * (states - self.lower_bounds) / self.range - 1\n",
    "        #Example: lower-upper is [0,1]. So normalize(0.5) = 2 * (0.5 - 0) /1 -1 = 0. Ok correct\n",
    "        #Another example. lower-upper is [0,30]. Sn normalize 15= 2 * 15 / 30 -1 = 0 Ok good.\n",
    "        # And normalize (20) = 40/30 - 1 = 1/3 yup\n",
    "        # Now denormalize(1/3) = 0.5 ( 1/3 +1 ) * 30 + 0 = 2/3*30 = 20\n",
    "        \n",
    "    def denormalize(self, normalized_states):\n",
    "        \"\"\"Convert normalized states back to original range\"\"\"\n",
    "        return 0.5 * (normalized_states + 1) * self.range + self.lower_bounds\n",
    "class ValueFunctionNN(nn.Module):\n",
    "    \"\"\"Neural network to approximate the value function\"\"\"\n",
    "    def __init__(self, state_dim, hidden_dims=[40, 30, 20, 10]):\n",
    "        super(ValueFunctionNN, self).__init__()\n",
    "        \n",
    "        # Build layers\n",
    "        layers = []\n",
    "        input_dim = state_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.Softplus())  # SiLU activation function\n",
    "            # Consider adding layer normalization for stability\n",
    "            #layers.append(nn.LayerNorm(hidden_dim))\n",
    "            input_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(input_dim, 1)) #was input_dim instead of 16\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "205077dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ContinuousContract import ContinuousContract\n",
    "cc=ContinuousContract(p)  \n",
    "cc.w_grid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2953918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_J,cc_W,_,cc_rho_star = cc.J(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48d4afbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.55541753e+02, -5.52232994e+02, -5.48924235e+02, -5.45615476e+02,\n",
       "       -5.42306717e+02, -5.38997958e+02, -5.35689199e+02, -5.32380440e+02,\n",
       "       -5.29071681e+02, -5.25762922e+02, -5.22454163e+02, -5.19145404e+02,\n",
       "       -5.15836646e+02, -5.12527887e+02, -5.09219128e+02, -5.05910369e+02,\n",
       "       -5.02601610e+02, -4.99292851e+02, -4.95984092e+02, -4.92675333e+02,\n",
       "       -4.89366574e+02, -4.86057815e+02, -4.82749056e+02, -4.79440297e+02,\n",
       "       -4.76131538e+02, -4.72822779e+02, -4.69514020e+02, -4.66205261e+02,\n",
       "       -4.62896502e+02, -4.59587743e+02, -4.56278985e+02, -4.52970226e+02,\n",
       "       -4.49661467e+02, -4.46352708e+02, -4.43043949e+02, -4.39735190e+02,\n",
       "       -4.36426431e+02, -4.33117672e+02, -4.29808913e+02, -4.26500154e+02,\n",
       "       -4.23191395e+02, -4.19882636e+02, -4.16573877e+02, -4.13265118e+02,\n",
       "       -4.09956359e+02, -4.06647600e+02, -4.03338841e+02, -4.00030083e+02,\n",
       "       -3.96721324e+02, -3.93412565e+02, -3.90103806e+02, -3.86795047e+02,\n",
       "       -3.83486288e+02, -3.80177529e+02, -3.76868770e+02, -3.73560011e+02,\n",
       "       -3.70251252e+02, -3.66942493e+02, -3.63633734e+02, -3.60324975e+02,\n",
       "       -3.57016216e+02, -3.53707457e+02, -3.50398698e+02, -3.47089939e+02,\n",
       "       -3.43781181e+02, -3.40472422e+02, -3.37163663e+02, -3.33854904e+02,\n",
       "       -3.30546145e+02, -3.27237386e+02, -3.23928627e+02, -3.20619868e+02,\n",
       "       -3.17311109e+02, -3.14002350e+02, -3.10693591e+02, -3.07384832e+02,\n",
       "       -3.04076073e+02, -3.00767314e+02, -2.97458555e+02, -2.94149796e+02,\n",
       "       -2.90841037e+02, -2.87532279e+02, -2.84223520e+02, -2.80914761e+02,\n",
       "       -2.77606002e+02, -2.74297243e+02, -2.70988484e+02, -2.67679725e+02,\n",
       "       -2.64370966e+02, -2.61062207e+02, -2.57753448e+02, -2.54444689e+02,\n",
       "       -2.51135930e+02, -2.47827171e+02, -2.44518412e+02, -2.41209653e+02,\n",
       "       -2.37900894e+02, -2.34592135e+02, -2.31283377e+02, -2.27974618e+02,\n",
       "       -2.24665859e+02, -2.21357100e+02, -2.18048341e+02, -2.14739582e+02,\n",
       "       -2.11430823e+02, -2.08122064e+02, -2.04813305e+02, -2.01504546e+02,\n",
       "       -1.98195787e+02, -1.94887028e+02, -1.91578269e+02, -1.88269510e+02,\n",
       "       -1.84960751e+02, -1.81651992e+02, -1.78343233e+02, -1.75034475e+02,\n",
       "       -1.71725716e+02, -1.68416957e+02, -1.65108198e+02, -1.61799439e+02,\n",
       "       -1.58490680e+02, -1.55181921e+02, -1.51873162e+02, -1.48564403e+02,\n",
       "       -1.45255644e+02, -1.41946885e+02, -1.38638126e+02, -1.35329367e+02,\n",
       "       -1.32020608e+02, -1.28711849e+02, -1.25403090e+02, -1.22094331e+02,\n",
       "       -1.18785573e+02, -1.15476814e+02, -1.12168055e+02, -1.08859296e+02,\n",
       "       -1.05550537e+02, -1.02241778e+02, -9.89330189e+01, -9.56242599e+01,\n",
       "       -9.23155010e+01, -8.90067420e+01, -8.56979831e+01, -8.23892242e+01,\n",
       "       -7.90804652e+01, -7.57717063e+01, -7.24629473e+01, -6.91541884e+01,\n",
       "       -6.58454294e+01, -6.25366705e+01, -5.92279116e+01, -5.59191526e+01,\n",
       "       -5.26103937e+01, -4.93016347e+01, -4.59928758e+01, -4.26841169e+01,\n",
       "       -3.93753579e+01, -3.60665990e+01, -3.27578400e+01, -2.94490811e+01,\n",
       "       -2.61403221e+01, -2.28315632e+01, -1.95228043e+01, -1.62140453e+01,\n",
       "       -1.29052864e+01, -9.59652744e+00, -6.28776850e+00, -2.97900956e+00,\n",
       "        3.29749386e-01,  3.63850833e+00,  6.94726727e+00,  1.02560262e+01,\n",
       "        1.35647852e+01,  1.68735441e+01,  2.01823030e+01,  2.34910620e+01,\n",
       "        2.67998209e+01,  3.01085799e+01,  3.34173388e+01,  3.67260977e+01,\n",
       "        4.00348567e+01,  4.33436156e+01,  4.66523746e+01,  4.99611335e+01,\n",
       "        5.32698925e+01,  5.65786514e+01,  5.98874103e+01,  6.31961693e+01,\n",
       "        6.65049282e+01,  6.98136872e+01,  7.31224461e+01,  7.64312050e+01,\n",
       "        7.97399640e+01,  8.30487229e+01,  8.63574819e+01,  8.96662408e+01,\n",
       "        9.29749998e+01,  9.62837587e+01,  9.95925176e+01,  1.02901277e+02])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc.v_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acdc7ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-8385.574702144228, 90.54769141022722, 3.199021364545065, 102.90127657534775)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_J.min(),cc_J.max(),cc_W.min(),cc_W.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd61e08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.03766514, 4.        , 7.85212434])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc.fun_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5254f37d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mcc\u001b[49m\u001b[38;5;241m.\u001b[39mrho_grid, cc_Jp[p\u001b[38;5;241m.\u001b[39mz_0\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]\u001b[38;5;241m+\u001b[39m cc\u001b[38;5;241m.\u001b[39mrho_grid[:] \u001b[38;5;241m*\u001b[39m cc_W[p\u001b[38;5;241m.\u001b[39mz_0\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:], label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVFI\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#Plot the policy functions\u001b[39;00m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cc' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(cc.rho_grid, cc_Jp[p.z_0-1,:]+ cc.rho_grid[:] * cc_W[p.z_0-1,:], label = \"VFI\")\n",
    "    #Plot the policy functions\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(cc.rho_grid, cc_W[p.z_0-1,:], label = \"VFI\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d0856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0738998",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FOCOptimizer:\n",
    "    \"\"\"\n",
    "    Class to solve first-order conditions given a state and value function\n",
    "    This is a placeholder - you'll need to implement actual FOC logic\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, value_function_model, bounds_processor, parameters=None, js=None):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.bounds_processor = bounds_processor  # Store bounds_processor\n",
    "\n",
    "        self.p = parameters\n",
    "        self.deriv_eps = 1e-4 # step size for derivative\n",
    "        # Model preferences initialized by the same parameter object.\n",
    "        self.pref = Preferences(input_param=self.p)\n",
    "\n",
    "        # Worker and Match Productivity Heterogeneity in the Model\n",
    "        self.Z_grid = self.construct_z_grid()   # Create match productivity grid\n",
    "\n",
    "        # Production Function in the Model\n",
    "        self.fun_prod = self.p.prod_a * np.power(self.Z_grid, self.p.prod_rho)\n",
    "        # Unemployment Benefits across Worker Productivities\n",
    "        self.unemp_bf = self.p.u_bf_m\n",
    "\n",
    "        # Transition matrices\n",
    "        self.Z_trans_mat = createPoissonTransitionMatrix(self.p.num_z, self.p.z_corr)\n",
    "\n",
    "        # Value Function Setup\n",
    "        self.J_grid   = -10 * np.ones((self.p.num_v)) #grid of job values, first productivity, then starting value, then tenure level\n",
    "        self.w_grid = np.linspace(self.unemp_bf, self.fun_prod.max(), self.p.num_v )\n",
    "        self.rho_grid=1/self.pref.utility_1d(self.w_grid)\n",
    "        # Normalize rho_grid to tensor for model input\n",
    "        self.rho_normalized = self.bounds_processor.normalize(torch.tensor(self.rho_grid, dtype=torch.float32)).unsqueeze(1)\n",
    "\n",
    "        \n",
    "        #Gotta fix the tightness+re functions somehow. Ultra simple J maybe?\n",
    "        self.v_grid=np.linspace(np.divide(self.pref.utility(self.unemp_bf),1-self.p.beta), np.divide(self.pref.utility(self.fun_prod.max()),1-self.p.beta), self.p.num_v ) #grid of submarkets the worker could theoretically search in. only used here for simplicity!!!\n",
    "        self.simple_J=np.divide(self.fun_prod[self.p.z_0-1,ax] -self.pref.inv_utility(self.v_grid[:]*(1-self.p.beta)),1-self.p.beta)\n",
    "        self.simple_Rho = self.simple_J + self.rho_grid * self.v_grid #We do indeed need to work with Rho here since we're taking W via its derivatives\n",
    "        #Apply the matching function: take the simple function and consider its different values across v.\n",
    "        self.prob_find_vx = self.p.alpha * np.power(1 - np.power(\n",
    "            np.divide(self.p.kappa, np.maximum(self.simple_J[ :], 1.0)), self.p.sigma), 1/self.p.sigma)\n",
    "        #Now get workers' probability to find a job while at some current value, as well as their return probabilities.\n",
    "        if js is None:\n",
    "            self.js = JobSearchArray() #Andrei: note that for us this array will have only one element\n",
    "            self.js.update(self.v_grid[:], self.prob_find_vx) #Andrei: two inputs: worker's value at the match quality of entrance (z_0-1), and the job-finding probability for the whole market\n",
    "        else:\n",
    "            self.js = js\n",
    "        #Note: I think??? js takes the values over the uniform grid only. so if I use NNs, gotta adapt it. But for now forget about updating it, keep it as is\n",
    "\n",
    "    def getWorkerDecisions(self, EW1, employed=True): #Andrei: Solves for the entire matrices of EW1 and EU\n",
    "        \"\"\"\n",
    "        :param EW1: Expected value of employment\n",
    "        :param EU:  Expected value of unemployment\n",
    "        :param employed: whether the worker is employed (in which case we multiply by efficiency)\n",
    "        :return: pe,re,qi search decision and associated return, as well as quit decision.\n",
    "        \"\"\"\n",
    "        pe, re = self.js.solve_search_choice(EW1) #Uses the job search array to solve for the search choice\n",
    "        assert (~np.isnan(pe)).all(), \"pe is not NaN\"\n",
    "        assert (pe <= 1).all(), \"pe is not less than 1\"\n",
    "        assert (pe >= -1e-10).all(), \"pe is not larger than 0\"\n",
    "        ve = self.js.ve(EW1)\n",
    "        if employed:\n",
    "            pe = pe * self.p.s_job\n",
    "            re = re * self.p.s_job\n",
    "        #print(\"Shape of pe:\", pe.shape)\n",
    "        # construct the continuation probability. #Andrei: probability the worker doesn't get fired and also doesn't leave\n",
    "        pc = (1 - pe)\n",
    "\n",
    "        return ve, re, pc #ve is vhat, the value the worker gets upon finding a job\n",
    "    def matching_function(self,J1): #Andrei: the formula of their matching function, applied to each particula job value J1\n",
    "        return self.p.alpha * np.power(1 - np.power(\n",
    "            np.divide(self.p.kappa, np.maximum(J1, self.p.kappa)), self.p.sigma),\n",
    "                                1 / self.p.sigma)\n",
    "    def construct_z_grid(self):\n",
    "        \"\"\"\n",
    "            Construct a grid for match productivity heterogeneity.\n",
    "        \"\"\"\n",
    "\n",
    "        exp_z = np.tile(np.linspace(0, 1, self.p.num_z + 2)[1:-1][:],\n",
    "                        (1))\n",
    "\n",
    "        return lnorm.ppf(q=exp_z, s=self.p.prod_var_z)    \n",
    "    def solve_foc(self, states, value_function_model):\n",
    "        \"\"\"\n",
    "        Solves first-order conditions to find optimal action and next state\n",
    "        \n",
    "        Args:\n",
    "            state: Current state tensor\n",
    "            value_function_model: Neural network model for value function\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with optimal action, next state, and immediate reward\n",
    "        \"\"\"\n",
    "        # This is a placeholder - replace with your actual FOC solver\n",
    "        # In a real implementation, you would:\n",
    "        # 1. Set up an optimization problem to find action that maximizes reward + discounted future value\n",
    "        # 2. Use value_function_model to evaluate future values\n",
    "        # 3. Return optimal action and resulting next state\n",
    "        # Compute gradient with gradients enabled\n",
    "        EW=self.EW\n",
    "        with torch.no_grad():\n",
    "            self.vf_output = value_function_model(self.rho_normalized).squeeze(1).numpy() #This EJpi from the CC FOC. I just precompute it for every point here\n",
    "        # Placeholder implementation (just random actions and states)\n",
    "        with torch.no_grad():\n",
    "            states_denorm = self.bounds_processor.denormalize(states).numpy()\n",
    "            # get worker decisions\n",
    "            _, _, pc = self.getWorkerDecisions(EW) #This EW1i is computed by taking the derivative of Rho, which is our core value function, wrt rho, which is the value-related state-variable\n",
    "            # get worker decisions at EW1i + epsilon\n",
    "            _, _, pc_d = self.getWorkerDecisions(EW + self.deriv_eps)\n",
    "            log_diff = np.zeros_like(self.rho_grid)\n",
    "            log_diff[:] = np.nan\n",
    "            log_diff[pc > 0] = np.log(pc_d[pc > 0]) - np.log(pc[pc > 0]) #This is log derivative of pc wrt the promised value\n",
    "            foc = self.rho_grid[:] - self.vf_output * log_diff / self.deriv_eps #So the FOC wrt promised value is: pay shadow cost lambda today (rho_grid), but more likely that the worker stays tomorrow\n",
    "            assert (np.isnan(foc) & (pc > 0)).sum() == 0, \"foc has NaN values where p>0\"\n",
    "\n",
    "                    #Andrei: so we look for the shadow cost that will satisfy the foc? Yes, look for u'(w'), with u'(w) given, so that the foc is satisfied\n",
    "                    # look for FOC below  rho_0\n",
    "\n",
    "            rho_star = np.interp(states_denorm,\n",
    "                                        impose_increasing(foc),\n",
    "                                        self.rho_grid)\n",
    "            rho_star_tensor = torch.tensor(rho_star, dtype=torch.float32)\n",
    "\n",
    "            action = rho_star_tensor\n",
    "            next_state = self.bounds_processor.normalize(rho_star_tensor)\n",
    "            reward = self.fun_prod[p.z_0-1] - np.interp(rho_star,self.rho_grid,self.w_grid) + states_denorm * self.pref.utility(np.interp(rho_star,self.rho_grid,self.w_grid))  # The entire Rho here. Big note though: this should be today's W, not EW\n",
    "            reward = torch.tensor(reward, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"action\": action,\n",
    "            \"next_state\": next_state,\n",
    "            \"reward\": reward\n",
    "        }\n",
    "    def get_batch_gradients(self,states, value_function_model):\n",
    "        states = states.detach().clone().requires_grad_(True)\n",
    "        values = value_function_model(states)\n",
    "    \n",
    "        # Sum values to get scalar for backward pass\n",
    "        values.sum().backward(retain_graph=True)\n",
    "    \n",
    "        gradients = states.grad.clone()\n",
    "        # Clear gradients for next computation\n",
    "        states.grad.zero_()\n",
    "        gradients = gradients / (0.5 * self.bounds_processor.range) #Normalize the gradient back to the original (not [-1,1]) state space\n",
    "\n",
    "        return gradients    \n",
    "    def get_value_function_gradient(self,state, value_function_model):\n",
    "        # Ensure state requires gradients\n",
    "        #if not state.requires_grad:\n",
    "        state = state.detach().clone().requires_grad_(True)\n",
    "    \n",
    "        # Forward pass\n",
    "        value = value_function_model(state)\n",
    "    \n",
    "        # Compute gradient of value with respect to state\n",
    "        #value.retain_grad()\n",
    "        value.sum().backward(retain_graph=True)\n",
    "\n",
    "        # Extract gradient\n",
    "        gradient = state.grad.clone()\n",
    "    \n",
    "        # Clear gradients for next computation\n",
    "        state.grad.zero_()\n",
    "        gradient = gradient / (0.5 * self.bounds_processor.range) #Normalize the gradient back to the original (not [-1,1]) state space\n",
    "        return gradient\n",
    "    #This can definitely be vectorized. So far each trajectory is done completely separately, even though the operations at each step are the same, no?\n",
    "    def simulate_trajectory(self,state, value_function_model, foc_optimizer, steps=5):\n",
    "        \"\"\"\n",
    "        Simulate a trajectory starting from a state and using the current value function\n",
    "    \n",
    "        Args:\n",
    "            state: Starting state tensor\n",
    "            value_function_model: Current value function model\n",
    "            foc_optimizer: Optimizer to solve FOCs\n",
    "            steps: Number of steps to simulate\n",
    "        \n",
    "        Returns:\n",
    "            Total discounted reward and final state value\n",
    "        \"\"\"\n",
    "        total_reward = 0\n",
    "        discount = 1.0\n",
    "    \n",
    "        current_state = state.clone()\n",
    "        with torch.enable_grad():\n",
    "            EW_tensor = self.get_batch_gradients(self.rho_normalized, value_function_model)[:,0] #This isn't even correct! I should be taking this at all the rho's!\n",
    "        self.EW = EW_tensor.detach().numpy()  # Convert to NumPy for further processing\n",
    "        for _ in range(steps):\n",
    "            \n",
    "            # Solve FOC to get optimal action and next state\n",
    "            result = foc_optimizer.solve_foc(current_state, value_function_model)\n",
    "            #Probability that the worker stays\n",
    "            EW_star = self.get_batch_gradients(result[\"next_state\"].unsqueeze(1).requires_grad_(True), value_function_model)[:,0]\n",
    "            ve_star, __, pc_star = self.getWorkerDecisions(EW_star.numpy())\n",
    "            ve_star = torch.from_numpy(ve_star)\n",
    "            pc_star = torch.from_numpy(pc_star)            \n",
    "            # Accumulate discounted reward\n",
    "            if _<range(steps)[-1]:\n",
    "                total_reward += discount * result[\"reward\"] + self.p.beta * self.bounds_processor.denormalize(current_state) * ( 1 - pc_star) * ve_star  #This should be: reward + rho*beta*ve_star*pe_star (so the value worker gets from leaving) + discount*next_reward\n",
    "            else:\n",
    "                total_reward += discount * result[\"reward\"] #for the last period we don't enymore do ve_star as that's included... right? confirm later\n",
    "            # Update state and discount factor\n",
    "            current_state = result[\"next_state\"]\n",
    "            discount *= self.p.beta * pc_star\n",
    "    \n",
    "        # Add final state value\n",
    "        with torch.no_grad():\n",
    "            final_value = value_function_model(current_state.unsqueeze(1).requires_grad_(True))\n",
    "    \n",
    "        total_value = total_reward + discount * final_value\n",
    "    \n",
    "        return total_value, current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d534e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_value_function(\n",
    "    state_dim,\n",
    "    lower_bounds,\n",
    "    upper_bounds,\n",
    "    action_dim=5,\n",
    "    hidden_dims=[40, 30, 20, 10],\n",
    "    num_iterations=20, \n",
    "    starting_points_per_iter=100,\n",
    "    simulation_steps=5,\n",
    "    learning_rate=0.001,\n",
    "    parameters=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Main training loop for value function approximation\n",
    "    \n",
    "    Args:\n",
    "        state_dim: Dimension of state space\n",
    "        action_dim: Dimension of action space\n",
    "        hidden_dims: List of hidden layer dimensions\n",
    "        num_iterations: Number of training iterations\n",
    "        starting_points_per_iter: Number of starting points per iteration\n",
    "        simulation_steps: Steps to simulate for each starting point\n",
    "        learning_rate: Learning rate for neural network optimizer\n",
    "        discount_factor: Discount factor for future rewards\n",
    "    \n",
    "    Returns:\n",
    "        Trained value function model\n",
    "    \"\"\"\n",
    "    bounds_processor = StateBoundsProcessor(lower_bounds,upper_bounds)\n",
    "    # Initialize value function neural network\n",
    "    value_function_model = ValueFunctionNN(state_dim, hidden_dims)\n",
    "    from ContinuousContract import ContinuousContract\n",
    "    cc=ContinuousContract(p)\n",
    "    # Initialize FOC optimizer\n",
    "    foc_optimizer = FOCOptimizer(state_dim, action_dim, value_function_model, bounds_processor, parameters, cc.js)\n",
    "    \n",
    "    # Initialize neural network optimizer\n",
    "    optimizer = optim.Adam(value_function_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    #Step 0: basic guess    \n",
    "    cc_J,cc_W,_,_ = cc.J(0)\n",
    "    target_values = torch.tensor(cc_J[p.z_0-1,:] + cc.rho_grid * cc_W[p.z_0-1,:], dtype=torch.float32)\n",
    "    states=bounds_processor.normalize(torch.tensor(foc_optimizer.rho_grid, dtype=torch.float32)).unsqueeze(1).requires_grad_(True) #This should be renormalized... right?\n",
    "    #print(np.max(np.abs(cc.rho_grid-foc_optimizer.rho_grid)))\n",
    "    #target_values=torch.tensor(foc_optimizer.simple_Rho, dtype=torch.float32)\n",
    "    for _ in (range(50)):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_values = value_function_model(states)[:,0]\n",
    "        loss = nn.MSELoss()(predicted_values, target_values)\n",
    "        loss.backward() #\n",
    "        optimizer.step()\n",
    "\n",
    "    # Training loop\n",
    "    for iteration in tqdm(range(num_iterations)):\n",
    "        # Generate uniform random starting states\n",
    "        states_denormal = torch.rand(starting_points_per_iter, state_dim,dtype=torch.float32) * bounds_processor.range + torch.tensor(lower_bounds,dtype=torch.float32) #Turning lower_bounds into torch every time is wasteful\n",
    "        states = bounds_processor.normalize(states_denormal).requires_grad_(True)\n",
    "        #print(\"States shape\", states.shape)\n",
    "        # Calculate target values through simulation\n",
    "        target_values = []\n",
    "\n",
    "        # Simulate trajectory and get total discounted reward\n",
    "        target_values, _ = foc_optimizer.simulate_trajectory(\n",
    "            states[:,0], value_function_model, foc_optimizer, simulation_steps)\n",
    "        #target_values.append(total_values)\n",
    "        \n",
    "        #target_values = torch.tensor(target_values, dtype=torch.float32)\n",
    "        \n",
    "        # Update neural network based on simulated values\n",
    "        optimizer.zero_grad()\n",
    "        predicted_values = value_function_model(states)\n",
    "        loss = nn.MSELoss()(predicted_values, target_values)\n",
    "        loss.backward() #\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print progress\n",
    "        if (iteration + 1) % 5 == 0 or iteration == 0:\n",
    "            print(f\"Iteration {iteration + 1}, Loss: {loss.item():.6f}\")\n",
    "    \n",
    "    return value_function_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f041660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_value_function(model, p, lower_bounds,upper_bounds):\n",
    "    \"\"\"\n",
    "    Evaluate the trained value function on test points\n",
    "    \n",
    "    Args:\n",
    "        model: Trained value function model\n",
    "        num_test_points: Number of test points\n",
    "        state_dim: Dimension of state space\n",
    "    \"\"\"\n",
    "\n",
    "    bounds_processor = StateBoundsProcessor(lower_bounds,upper_bounds)\n",
    "    # Generate random test states\n",
    "    #test_states = torch.randn(num_test_points, state_dim)\n",
    "    from ContinuousContract import ContinuousContract\n",
    "    cc=ContinuousContract(p)    \n",
    "    cc_J,cc_W,_,_ = cc.J(0)\n",
    "    cc_Rho = cc_J[p.z_0-1,:] + cc.rho_grid * cc_W[p.z_0-1,:]\n",
    "    test_states = bounds_processor.normalize(torch.tensor(cc.rho_grid, dtype=torch.float32)).unsqueeze(1)\n",
    "    # Evaluate model\n",
    "    with torch.no_grad():\n",
    "        values = model(test_states)\n",
    "    \n",
    "    # Print results\n",
    "    #print(\"\\nValue function evaluation on test states:\")\n",
    "    #for i in range(min(5, num_test_points)):\n",
    "    #    print(f\"State {i+1}: Value = {values[i].item():.4f}\")\n",
    "    #Plot results\n",
    "    plt.plot(cc.rho_grid, cc_Rho, label = \"VFI\")\n",
    "    plt.plot(cc.rho_grid, values, label = \"NN\")    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ceb00c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds_processor = StateBoundsProcessor(LOWER_BOUNDS,UPPER_BOUNDS)\n",
    "foc_optimizer = FOCOptimizer(STATE_DIM, ACTION_DIM, trained_model, bounds_processor, p, cc.js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5097f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000e+00],\n",
       "        [-9.9774e-01],\n",
       "        [-9.9461e-01],\n",
       "        [-9.9080e-01],\n",
       "        [-9.8643e-01],\n",
       "        [-9.8156e-01],\n",
       "        [-9.7623e-01],\n",
       "        [-9.7049e-01],\n",
       "        [-9.6435e-01],\n",
       "        [-9.5785e-01],\n",
       "        [-9.5099e-01],\n",
       "        [-9.4381e-01],\n",
       "        [-9.3632e-01],\n",
       "        [-9.2851e-01],\n",
       "        [-9.2042e-01],\n",
       "        [-9.1205e-01],\n",
       "        [-9.0340e-01],\n",
       "        [-8.9449e-01],\n",
       "        [-8.8532e-01],\n",
       "        [-8.7591e-01],\n",
       "        [-8.6625e-01],\n",
       "        [-8.5635e-01],\n",
       "        [-8.4622e-01],\n",
       "        [-8.3586e-01],\n",
       "        [-8.2529e-01],\n",
       "        [-8.1449e-01],\n",
       "        [-8.0349e-01],\n",
       "        [-7.9227e-01],\n",
       "        [-7.8085e-01],\n",
       "        [-7.6923e-01],\n",
       "        [-7.5741e-01],\n",
       "        [-7.4540e-01],\n",
       "        [-7.3319e-01],\n",
       "        [-7.2080e-01],\n",
       "        [-7.0822e-01],\n",
       "        [-6.9546e-01],\n",
       "        [-6.8251e-01],\n",
       "        [-6.6939e-01],\n",
       "        [-6.5610e-01],\n",
       "        [-6.4263e-01],\n",
       "        [-6.2899e-01],\n",
       "        [-6.1518e-01],\n",
       "        [-6.0120e-01],\n",
       "        [-5.8706e-01],\n",
       "        [-5.7276e-01],\n",
       "        [-5.5829e-01],\n",
       "        [-5.4366e-01],\n",
       "        [-5.2888e-01],\n",
       "        [-5.1394e-01],\n",
       "        [-4.9885e-01],\n",
       "        [-4.8360e-01],\n",
       "        [-4.6821e-01],\n",
       "        [-4.5266e-01],\n",
       "        [-4.3696e-01],\n",
       "        [-4.2112e-01],\n",
       "        [-4.0513e-01],\n",
       "        [-3.8900e-01],\n",
       "        [-3.7272e-01],\n",
       "        [-3.5631e-01],\n",
       "        [-3.3975e-01],\n",
       "        [-3.2305e-01],\n",
       "        [-3.0621e-01],\n",
       "        [-2.8924e-01],\n",
       "        [-2.7213e-01],\n",
       "        [-2.5489e-01],\n",
       "        [-2.3751e-01],\n",
       "        [-2.2000e-01],\n",
       "        [-2.0236e-01],\n",
       "        [-1.8458e-01],\n",
       "        [-1.6668e-01],\n",
       "        [-1.4864e-01],\n",
       "        [-1.3048e-01],\n",
       "        [-1.1219e-01],\n",
       "        [-9.3780e-02],\n",
       "        [-7.5240e-02],\n",
       "        [-5.6575e-02],\n",
       "        [-3.7786e-02],\n",
       "        [-1.8875e-02],\n",
       "        [ 1.5938e-04],\n",
       "        [ 1.9315e-02],\n",
       "        [ 3.8591e-02],\n",
       "        [ 5.7986e-02],\n",
       "        [ 7.7502e-02],\n",
       "        [ 9.7135e-02],\n",
       "        [ 1.1689e-01],\n",
       "        [ 1.3675e-01],\n",
       "        [ 1.5674e-01],\n",
       "        [ 1.7684e-01],\n",
       "        [ 1.9706e-01],\n",
       "        [ 2.1738e-01],\n",
       "        [ 2.3783e-01],\n",
       "        [ 2.5839e-01],\n",
       "        [ 2.7905e-01],\n",
       "        [ 2.9984e-01],\n",
       "        [ 3.2073e-01],\n",
       "        [ 3.4173e-01],\n",
       "        [ 3.6284e-01],\n",
       "        [ 3.8407e-01],\n",
       "        [ 4.0540e-01],\n",
       "        [ 4.2684e-01],\n",
       "        [ 4.4839e-01],\n",
       "        [ 4.7004e-01],\n",
       "        [ 4.9180e-01],\n",
       "        [ 5.1367e-01],\n",
       "        [ 5.3565e-01],\n",
       "        [ 5.5772e-01],\n",
       "        [ 5.7991e-01],\n",
       "        [ 6.0220e-01],\n",
       "        [ 6.2459e-01],\n",
       "        [ 6.4708e-01],\n",
       "        [ 6.6968e-01],\n",
       "        [ 6.9238e-01],\n",
       "        [ 7.1519e-01],\n",
       "        [ 7.3809e-01],\n",
       "        [ 7.6110e-01],\n",
       "        [ 7.8420e-01],\n",
       "        [ 8.0741e-01],\n",
       "        [ 8.3071e-01],\n",
       "        [ 8.5412e-01],\n",
       "        [ 8.7762e-01],\n",
       "        [ 9.0122e-01],\n",
       "        [ 9.2492e-01],\n",
       "        [ 9.4872e-01],\n",
       "        [ 9.7262e-01],\n",
       "        [ 9.9661e-01],\n",
       "        [ 1.0207e+00],\n",
       "        [ 1.0449e+00],\n",
       "        [ 1.0692e+00],\n",
       "        [ 1.0935e+00],\n",
       "        [ 1.1180e+00],\n",
       "        [ 1.1426e+00],\n",
       "        [ 1.1672e+00],\n",
       "        [ 1.1920e+00],\n",
       "        [ 1.2168e+00],\n",
       "        [ 1.2418e+00],\n",
       "        [ 1.2668e+00],\n",
       "        [ 1.2919e+00],\n",
       "        [ 1.3171e+00],\n",
       "        [ 1.3425e+00],\n",
       "        [ 1.3679e+00],\n",
       "        [ 1.3933e+00],\n",
       "        [ 1.4189e+00],\n",
       "        [ 1.4446e+00],\n",
       "        [ 1.4704e+00],\n",
       "        [ 1.4962e+00],\n",
       "        [ 1.5222e+00],\n",
       "        [ 1.5482e+00],\n",
       "        [ 1.5743e+00],\n",
       "        [ 1.6005e+00],\n",
       "        [ 1.6268e+00],\n",
       "        [ 1.6532e+00],\n",
       "        [ 1.6797e+00],\n",
       "        [ 1.7062e+00],\n",
       "        [ 1.7329e+00],\n",
       "        [ 1.7596e+00],\n",
       "        [ 1.7864e+00],\n",
       "        [ 1.8133e+00],\n",
       "        [ 1.8403e+00],\n",
       "        [ 1.8674e+00],\n",
       "        [ 1.8946e+00],\n",
       "        [ 1.9218e+00],\n",
       "        [ 1.9492e+00],\n",
       "        [ 1.9766e+00],\n",
       "        [ 2.0041e+00],\n",
       "        [ 2.0317e+00],\n",
       "        [ 2.0593e+00],\n",
       "        [ 2.0871e+00],\n",
       "        [ 2.1149e+00],\n",
       "        [ 2.1429e+00],\n",
       "        [ 2.1709e+00],\n",
       "        [ 2.1990e+00],\n",
       "        [ 2.2271e+00],\n",
       "        [ 2.2554e+00],\n",
       "        [ 2.2837e+00],\n",
       "        [ 2.3121e+00],\n",
       "        [ 2.3406e+00],\n",
       "        [ 2.3692e+00],\n",
       "        [ 2.3979e+00],\n",
       "        [ 2.4266e+00],\n",
       "        [ 2.4554e+00],\n",
       "        [ 2.4843e+00],\n",
       "        [ 2.5133e+00],\n",
       "        [ 2.5424e+00],\n",
       "        [ 2.5715e+00],\n",
       "        [ 2.6007e+00],\n",
       "        [ 2.6300e+00],\n",
       "        [ 2.6594e+00],\n",
       "        [ 2.6889e+00],\n",
       "        [ 2.7184e+00],\n",
       "        [ 2.7480e+00],\n",
       "        [ 2.7777e+00],\n",
       "        [ 2.8075e+00],\n",
       "        [ 2.8373e+00],\n",
       "        [ 2.8673e+00],\n",
       "        [ 2.8973e+00],\n",
       "        [ 2.9273e+00],\n",
       "        [ 2.9575e+00],\n",
       "        [ 2.9877e+00],\n",
       "        [ 3.0181e+00],\n",
       "        [ 3.0484e+00]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foc_optimizer.rho_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c649c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(1, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(3)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0769e0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training value function...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/690 [00:00<?, ?it/s]c:\\Programs\\Python 3.11\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([200, 200])) that is different to the input size (torch.Size([200, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "  1%|          | 8/690 [00:00<00:08, 76.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Loss: 1072808.125000\n",
      "Iteration 5, Loss: 1042377.750000\n",
      "Iteration 10, Loss: 1010504.500000\n",
      "Iteration 15, Loss: 998502.500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 42/690 [00:00<00:06, 106.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, Loss: 976151.125000\n",
      "Iteration 25, Loss: 907700.812500\n",
      "Iteration 30, Loss: 908379.062500\n",
      "Iteration 35, Loss: 927538.562500\n",
      "Iteration 40, Loss: 969004.625000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 53/690 [00:00<00:06, 106.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45, Loss: 953240.562500\n",
      "Iteration 50, Loss: 886677.312500\n",
      "Iteration 55, Loss: 778477.437500\n",
      "Iteration 60, Loss: 834091.187500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 76/690 [00:00<00:05, 103.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 65, Loss: 724189.000000\n",
      "Iteration 70, Loss: 778527.312500\n",
      "Iteration 75, Loss: 717930.187500\n",
      "Iteration 80, Loss: 628456.937500\n",
      "Iteration 85, Loss: 601701.625000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|        | 111/690 [00:01<00:05, 108.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 90, Loss: 581074.625000\n",
      "Iteration 95, Loss: 483252.218750\n",
      "Iteration 100, Loss: 525326.562500\n",
      "Iteration 105, Loss: 407148.187500\n",
      "Iteration 110, Loss: 421952.343750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|        | 133/690 [00:01<00:05, 95.62it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 115, Loss: 434463.125000\n",
      "Iteration 120, Loss: 336088.781250\n",
      "Iteration 125, Loss: 359288.968750\n",
      "Iteration 130, Loss: 326045.500000\n",
      "Iteration 135, Loss: 307162.406250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 154/690 [00:01<00:05, 99.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 140, Loss: 277218.156250\n",
      "Iteration 145, Loss: 250334.921875\n",
      "Iteration 150, Loss: 285057.375000\n",
      "Iteration 155, Loss: 254789.578125\n",
      "Iteration 160, Loss: 248147.125000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 176/690 [00:01<00:05, 95.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 165, Loss: 245808.890625\n",
      "Iteration 170, Loss: 248160.593750\n",
      "Iteration 175, Loss: 248967.468750\n",
      "Iteration 180, Loss: 285348.187500\n",
      "Iteration 185, Loss: 243185.203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 197/690 [00:01<00:05, 98.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 190, Loss: 264384.375000\n",
      "Iteration 195, Loss: 250731.468750\n",
      "Iteration 200, Loss: 276909.718750\n",
      "Iteration 205, Loss: 262691.750000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 218/690 [00:02<00:05, 90.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 210, Loss: 267776.125000\n",
      "Iteration 215, Loss: 292850.031250\n",
      "Iteration 220, Loss: 272304.531250\n",
      "Iteration 225, Loss: 281415.843750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 250/690 [00:02<00:04, 97.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 230, Loss: 247926.687500\n",
      "Iteration 235, Loss: 253134.343750\n",
      "Iteration 240, Loss: 261770.296875\n",
      "Iteration 245, Loss: 271342.343750\n",
      "Iteration 250, Loss: 251285.140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|      | 270/690 [00:02<00:04, 88.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 255, Loss: 260978.812500\n",
      "Iteration 260, Loss: 266408.750000\n",
      "Iteration 265, Loss: 235421.078125\n",
      "Iteration 270, Loss: 270623.125000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 293/690 [00:03<00:04, 97.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 275, Loss: 266849.187500\n",
      "Iteration 280, Loss: 248010.468750\n",
      "Iteration 285, Loss: 272574.937500\n",
      "Iteration 290, Loss: 256446.484375\n",
      "Iteration 295, Loss: 260139.265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|     | 313/690 [00:03<00:04, 93.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 300, Loss: 256643.609375\n",
      "Iteration 305, Loss: 241915.390625\n",
      "Iteration 310, Loss: 269845.593750\n",
      "Iteration 315, Loss: 255845.328125\n",
      "Iteration 320, Loss: 273088.781250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|     | 335/690 [00:03<00:03, 100.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 325, Loss: 273935.281250\n",
      "Iteration 330, Loss: 254117.843750\n",
      "Iteration 335, Loss: 265175.656250\n",
      "Iteration 340, Loss: 227105.593750\n",
      "Iteration 345, Loss: 249321.171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 370/690 [00:03<00:03, 100.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 350, Loss: 272571.343750\n",
      "Iteration 355, Loss: 292193.125000\n",
      "Iteration 360, Loss: 266058.687500\n",
      "Iteration 365, Loss: 272446.468750\n",
      "Iteration 370, Loss: 249512.421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|    | 393/690 [00:03<00:02, 103.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 375, Loss: 265883.968750\n",
      "Iteration 380, Loss: 244591.828125\n",
      "Iteration 385, Loss: 280150.875000\n",
      "Iteration 390, Loss: 237677.468750\n",
      "Iteration 395, Loss: 270500.218750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 416/690 [00:04<00:02, 106.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 400, Loss: 245079.703125\n",
      "Iteration 405, Loss: 283764.062500\n",
      "Iteration 410, Loss: 272286.000000\n",
      "Iteration 415, Loss: 245817.296875\n",
      "Iteration 420, Loss: 229322.906250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|   | 438/690 [00:04<00:02, 103.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 425, Loss: 269806.468750\n",
      "Iteration 430, Loss: 238455.187500\n",
      "Iteration 435, Loss: 281074.531250\n",
      "Iteration 440, Loss: 286969.593750\n",
      "Iteration 445, Loss: 286843.968750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|   | 463/690 [00:04<00:02, 109.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 450, Loss: 229300.406250\n",
      "Iteration 455, Loss: 273307.687500\n",
      "Iteration 460, Loss: 255077.906250\n",
      "Iteration 465, Loss: 279008.781250\n",
      "Iteration 470, Loss: 245204.703125\n",
      "Iteration 475, Loss: 256197.734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|   | 488/690 [00:04<00:01, 108.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 480, Loss: 257915.187500\n",
      "Iteration 485, Loss: 252074.750000\n",
      "Iteration 490, Loss: 257332.890625\n",
      "Iteration 495, Loss: 248386.359375\n",
      "Iteration 500, Loss: 279221.343750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|  | 524/690 [00:05<00:01, 112.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 505, Loss: 278161.468750\n",
      "Iteration 510, Loss: 231339.921875\n",
      "Iteration 515, Loss: 273034.000000\n",
      "Iteration 520, Loss: 278587.781250\n",
      "Iteration 525, Loss: 243020.437500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 547/690 [00:05<00:01, 102.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 530, Loss: 271022.937500\n",
      "Iteration 535, Loss: 266065.406250\n",
      "Iteration 540, Loss: 248310.421875\n",
      "Iteration 545, Loss: 261431.093750\n",
      "Iteration 550, Loss: 259869.546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%| | 570/690 [00:05<00:01, 102.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 555, Loss: 269151.218750\n",
      "Iteration 560, Loss: 277035.812500\n",
      "Iteration 565, Loss: 258489.859375\n",
      "Iteration 570, Loss: 262350.375000\n",
      "Iteration 575, Loss: 269329.250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 594/690 [00:05<00:00, 105.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 580, Loss: 304275.343750\n",
      "Iteration 585, Loss: 258307.125000\n",
      "Iteration 590, Loss: 258928.843750\n",
      "Iteration 595, Loss: 244027.343750\n",
      "Iteration 600, Loss: 238008.109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 618/690 [00:06<00:00, 109.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 605, Loss: 250764.390625\n",
      "Iteration 610, Loss: 246388.843750\n",
      "Iteration 615, Loss: 266904.843750\n",
      "Iteration 620, Loss: 247580.140625\n",
      "Iteration 625, Loss: 248204.843750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 642/690 [00:06<00:00, 108.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 630, Loss: 256453.687500\n",
      "Iteration 635, Loss: 286861.093750\n",
      "Iteration 640, Loss: 263090.062500\n",
      "Iteration 645, Loss: 276633.156250\n",
      "Iteration 650, Loss: 285180.968750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|| 665/690 [00:06<00:00, 106.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 655, Loss: 276470.687500\n",
      "Iteration 660, Loss: 254664.609375\n",
      "Iteration 665, Loss: 250487.093750\n",
      "Iteration 670, Loss: 256669.187500\n",
      "Iteration 675, Loss: 255397.281250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 690/690 [00:06<00:00, 101.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 680, Loss: 265158.062500\n",
      "Iteration 685, Loss: 271582.218750\n",
      "Iteration 690, Loss: 280956.625000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to trained_value_function.pt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define parameters\n",
    "    STATE_DIM = 1  # Just one, continuous state, the promised value. Next step will be adding (discrete) y\n",
    "    ACTION_DIM = 1  # Adjust based on your problem\n",
    "    HIDDEN_DIMS = [64,64]  # Decreasing width architecture\n",
    "    pref = Preferences(input_param=p)\n",
    "    \n",
    "    LOWER_BOUNDS = [1/pref.utility_1d(p.u_bf_m)]\n",
    "    UPPER_BOUNDS = [1/pref.utility_1d(10)] #Ideally this should come from fun_prod.max\n",
    "    # Train value function\n",
    "    print(\"Training value function...\")\n",
    "    trained_model = train_value_function(\n",
    "        state_dim=STATE_DIM,\n",
    "        lower_bounds=LOWER_BOUNDS,\n",
    "        upper_bounds=UPPER_BOUNDS,\n",
    "        action_dim=ACTION_DIM,\n",
    "        hidden_dims=HIDDEN_DIMS,\n",
    "        num_iterations=690,\n",
    "        starting_points_per_iter=200,\n",
    "        simulation_steps=5,\n",
    "        learning_rate=0.003,\n",
    "        parameters=p\n",
    "    )\n",
    "    \n",
    "    # Evaluate trained model\n",
    "    evaluate_value_function(trained_model, p, LOWER_BOUNDS, UPPER_BOUNDS)\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(trained_model.state_dict(), \"trained_value_function.pt\")\n",
    "    print(\"Model saved to trained_value_function.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24a35ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3215.1104], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model(torch.tensor([-40.0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
